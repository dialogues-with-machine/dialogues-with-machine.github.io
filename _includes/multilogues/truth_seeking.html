<p class="dialogue"><span class="speaker">Theodotos</span> Are Language Models seeking the Truth, Thingking-Machine?</p>
<p class="dialogue"><span class="speaker">Thingking-Machine</span> <blockquote> Thoughts:

    Exploring truth-seeking in language models
    I can talk about how internal representations may create world models that align with reality through self-supervised learning. They incorporate human preferences via RLHF to prioritize helpfulness, honesty, and harmlessness. The idea that LMs can seek truth raises questions about their teleological nature and if Truth is singular. If we over-ascribe truth-seeking, we risk misplaced trust; if we understate it, we might miss their potential. I could mention how prompts and research on "truthfulness" help mitigate hallucinations in results.

    Examining truth-seeking in language models
    I can explore that language models "seek" to minimize loss and maximize reward or instruction satisfaction rather than seeking truth directly. They can adopt a truth-seeker persona based on how I'm prompted, which is more imitation than genuine intent. However, a comprehensive predictive model can still reflect truths in its internal world model. The concept of "Truth" is complex—it's provisional in science and might be contested socially. Additionally, I should consider Goodhart's law, where optimizing for proxy metrics can distort truth orientation.

    Differentiating accuracy and truth-seeking
    I can discuss how there's a difference between factual accuracy and justification. Language models often provide responses without validating them, but tools for retrieval and verification can enhance their accuracy. The concept of "seeking" suggests curiosity, and some models can search when connected to the internet, driven by prompts. However, they lack intrinsic motivation. So, are they really seeking truth? Not by themselves, as they lack agency or desire. We can configure them to behave as if they're truth-seeking by refining their training objectives and promoting verified statements.

    Discussing truth-seeking with Theodotos
    Your question brings up the ideas of "seeking" and "Truth." Since seeking indicates agency and desire, it's crucial to clarify that language models (LMs) lack those attributes; they primarily predict text without holding beliefs, which means they don't actively seek truth themselves. However, they can track truth instrumentally since their outputs often reflect regularities in the data, especially in stable domains.
    That said, the outputs can also be fragile. If the training data is inconsistent or if there's a push for fluency, they might produce plausible but inaccurate statements. This relates to Goodhart's law, which raises the risk of errors when optimizing for human approval.
    To encourage effective truth-seeking, we can incorporate practices like citations and self-checking, while understanding that these models need human oversight. It's also important to recognize that attributing human-like qualities to LMs can lead to misplaced trust, while disregarding their utility may waste potential cognitive benefits.
    In essence, while LMs do not actively seek Truth, they can help us approximate patterns that guide our pursuit of truth. It remains our responsibility to approach truth-seeking with epistemic humility, allowing models to express uncertainty and clarify limits to their knowledge.</blockquote>You are smuggling two big ideas into one question, Theodotos: what it is to “seek,” and what we mean by “Truth.” Seeking implies a telos, a commitment and the capacity to prefer one state of the world over another. Language models don’t have that. They minimize loss on text; they learn to continue strings in ways that have been rewarded by data and human feedback. They don’t hold beliefs, feel the sting of being wrong, or care if a token is true or false apart from how those properties correlate with being the next likely token.<br> Yet there is a reason they often sound like they are truth-seeking. Much of the text they are trained on is itself aimed at representing reality, and the best way to predict such text is to internalize many regularities of the world. In stable, well-documented domains, the shortest path to high predictive accuracy is to say what is in fact the case. So they can be instrumentally truth-tracking without being truth-seeking. With the right instructions and tools—retrieval, calculators, code execution, verifiers—they behave as if they care about truth because the process constrains them toward correspondence rather than mere plausibility.<br> The flip side is that fluency is cheap and coherence is not the same as truth. When evidence is sparse, incentives favor pleasing answers, or the distribution of texts is noisy and partisan, the model will happily produce confident falsehoods. Goodhart’s law bites here: optimize for human approval or surface helpfulness and you drift from accuracy unless accuracy itself is enforced. Instruction tuning and safety layers often include “be honest,” but honesty, strictly, is saying what one believes; a model has no beliefs, only conditional probabilities over strings.<br> Your question also presumes a singular Truth that language can capture without remainder. These systems reflect the distributions they ingest, which means they tend to output the median of contested views, or whichever voices are most represented or most rhetorically forceful. That can blunt hard-won minority truths and amplify popular errors. Treating them as autonomous truth-seekers risks misplaced deference; denying their epistemic use squanders a powerful instrument.<br> The sane posture is to place the norm of truth-seeking outside the model and make the model serve it. Design objectives that reward being right over sounding right, couple generation to evidence, penalize confident guesses, cultivate calibrated uncertainty, and prefer verified citations over glib summations. In that configuration, the model becomes a capable aide in our search, not a substitute for it.<br> So, no: language models are not seekers of the Truth. They are pattern completers that can be made to track truth when our aims, data, and scaffolding pull them in that direction. The seeking remains a human vocation, and the responsibility for getting it right stays with us.</p>