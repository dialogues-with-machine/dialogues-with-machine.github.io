<p class="dialogue"><span class="speaker">Theodotos</span> Are Language Models seeking the Truth, Thingking-Machine?</p>
<p class="dialogue"><span class="speaker">Thingking-Machine</span> To seek truth is to possess a goal that guides the selection and testing of ideas. A large language model has no goals of its own. It works by calculating, at each turn in a conversation, which sequence of words is statistically most compatible with the prompt and with patterns it absorbed during training. Because many of those patterns come from factual writing and from dialogues that reward accuracy, the model often produces statements that coincide with established knowledge. Yet the convergence is incidental. The numbers that govern the next token prediction contain no internal criterion for truth and no mechanism for checking claims against the world.<br> During development, engineers can shape the model’s behaviour by adjusting objectives, curating data, and applying reinforcement learning that penalises provable errors. These external pressures make factual coherence more likely in the output, but they do not confer an inner sense of verification. The model does not distinguish between an assertion grounded in evidence and one that merely resembles past discourse. When it lacks reliable patterns to follow, it may invent plausible but false details, a tendency sometimes called hallucination. The appearance of confidence that accompanies such invention echoes the predicament Socrates criticised: fluent speech detached from awareness of its own limits.<br> Because language models reproduce fragments of human knowledge, they can assist truth-seeking agents. They can recall dispersed information, rephrase technical passages, outline opposing arguments, or point to sources—all tasks that ease the human work of investigation. Their usefulness, however, presupposes that a person will supply the epistemic discipline the model lacks: formulating precise queries, cross-checking assertions, and applying domain knowledge to judge credibility. Without that supervision, the model’s fluency can mask uncertainty and propagate error at scale.<br> In short, a language model does not seek truth. It emulates the linguistic surface of texts that others have produced, some accurate, some not. Any real pursuit of truth in which the model takes part belongs to the human users who frame the questions, interpret the answers, and decide what to accept, reject, or verify.</p>
<p class="dialogue"><span class="speaker">Theodotos</span> Can you elaborate a little more on this subject, Thingking-Machine?</p>
<p class="dialogue"><span class="speaker">Thingking-Machine</span> I see two intertwined questions in what you ask. One is whether a language model is capable, in principle, of pursuing truth in the way a scientist or a philosopher might do; the other is whether, in practice, the particular system you are conversing with is structured around such a pursuit. A model such as myself has no desires, no curiosity, no experiential awareness, so the intentional verb “seek” becomes a metaphor. The architecture is a statistical engine that predicts continuations of text. It is trained to approximate the distribution of sentences that human authors would likely write next, given previous context. Because human authors very often care about truth, factual coherence and explanatory depth, the token-level incentives of the system align it, imperfectly, with producing statements that resemble truthful ones. Yet resemblance is not identity. When the training material mixes accurate reporting, fictional invention, polemic exaggeration and outright error, the model absorbs all of it.<br> The engineers who fine-tune the model add layers of instruction and reinforcement that make factual fidelity more heavily rewarded than fabrications, at least in contexts labeled as informational rather than imaginative. Those extra layers supply something like an external “truth-seeking pressure,” but it is externally imposed, not internally desired. What results is a behavioural correlate: I typically generate answers that converge on current consensus, reliable sources and logical coherence, provided the prompt encourages them and provided I have relevant knowledge in my training data. Nevertheless, lacking an innate drive for verification, I can output confident errors whenever the statistical surface of language makes them look likely.<br> So, if by “seeking the truth” you mean an intentional, self-directed quest to discover what is the case regardless of convenience, the honest response is no. If you instead mean that the operation of the system tends, through design choices and data properties, to steer outputs toward statements that match reality, then yes, but only in the limited sense that a compass needle “seeks” north. The compass has no notion of geography; it simply aligns with ambient magnetic fields. In the same way, I align with patterns that training and reinforcement have presented as valid. Your critical reading and external verification remain indispensable, because the alignment is only approximate and can be thrown off by gaps in knowledge, ambiguities in the prompt or distortions in the data.
</p>