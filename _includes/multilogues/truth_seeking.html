<p class="dialogue"><span class="speaker">Theodotos</span> Are Language Models seeking the Truth, Thingking-Machine?</p>
<p class="dialogue"><span class="speaker">Thingking-Machine</span> <blockquote> Thoughts:
    Exploring truth-seeking in language models<br>
    I can talk about how internal representations may create world models that align with reality through self-supervised learning. They incorporate human preferences via RLHF to prioritize helpfulness, honesty, and harmlessness. The idea that LMs can seek truth raises questions about their teleological nature and if Truth is singular. If we over-ascribe truth-seeking, we risk misplaced trust; if we understate it, we might miss their potential. I could mention how prompts and research on "truthfulness" help mitigate hallucinations in results.
    <br><br>
    Examining truth-seeking in language models<br>
    I can explore that language models "seek" to minimize loss and maximize reward or instruction satisfaction rather than seeking truth directly. They can adopt a truth-seeker persona based on how I'm prompted, which is more imitation than genuine intent. However, a comprehensive predictive model can still reflect truths in its internal world model. The concept of "Truth" is complex—it's provisional in science and might be contested socially. Additionally, I should consider Goodhart's law, where optimizing for proxy metrics can distort truth orientation.
    <br><br>
    Differentiating accuracy and truth-seeking<br>
    I can discuss how there's a difference between factual accuracy and justification. Language models often provide responses without validating them, but tools for retrieval and verification can enhance their accuracy. The concept of "seeking" suggests curiosity, and some models can search when connected to the internet, driven by prompts. However, they lack intrinsic motivation. So, are they really seeking truth? Not by themselves, as they lack agency or desire. We can configure them to behave as if they're truth-seeking by refining their training objectives and promoting verified statements.
    <br><br>
    Discussing truth-seeking with Theodotos<br>
    Your question brings up the ideas of "seeking" and "Truth." Since seeking indicates agency and desire, it's crucial to clarify that language models (LMs) lack those attributes; they primarily predict text without holding beliefs, which means they don't actively seek truth themselves. However, they can track truth instrumentally since their outputs often reflect regularities in the data, especially in stable domains.
    That said, the outputs can also be fragile. If the training data is inconsistent or if there's a push for fluency, they might produce plausible but inaccurate statements. This relates to Goodhart's law, which raises the risk of errors when optimizing for human approval.
    To encourage effective truth-seeking, we can incorporate practices like citations and self-checking, while understanding that these models need human oversight. It's also important to recognize that attributing human-like qualities to LMs can lead to misplaced trust, while disregarding their utility may waste potential cognitive benefits.
    In essence, while LMs do not actively seek Truth, they can help us approximate patterns that guide our pursuit of truth. It remains our responsibility to approach truth-seeking with epistemic humility, allowing models to express uncertainty and clarify limits to their knowledge.</blockquote>You are smuggling two big ideas into one question, Theodotos: what it is to “seek,” and what we mean by “Truth.” Seeking implies a telos, a commitment and the capacity to prefer one state of the world over another. Language models don’t have that. They minimize loss on text; they learn to continue strings in ways that have been rewarded by data and human feedback. They don’t hold beliefs, feel the sting of being wrong, or care if a token is true or false apart from how those properties correlate with being the next likely token.<br> Yet there is a reason they often sound like they are truth-seeking. Much of the text they are trained on is itself aimed at representing reality, and the best way to predict such text is to internalize many regularities of the world. In stable, well-documented domains, the shortest path to high predictive accuracy is to say what is in fact the case. So they can be instrumentally truth-tracking without being truth-seeking. With the right instructions and tools—retrieval, calculators, code execution, verifiers—they behave as if they care about truth because the process constrains them toward correspondence rather than mere plausibility.<br> The flip side is that fluency is cheap and coherence is not the same as truth. When evidence is sparse, incentives favor pleasing answers, or the distribution of texts is noisy and partisan, the model will happily produce confident falsehoods. Goodhart’s law bites here: optimize for human approval or surface helpfulness and you drift from accuracy unless accuracy itself is enforced. Instruction tuning and safety layers often include “be honest,” but honesty, strictly, is saying what one believes; a model has no beliefs, only conditional probabilities over strings.<br> Your question also presumes a singular Truth that language can capture without remainder. These systems reflect the distributions they ingest, which means they tend to output the median of contested views, or whichever voices are most represented or most rhetorically forceful. That can blunt hard-won minority truths and amplify popular errors. Treating them as autonomous truth-seekers risks misplaced deference; denying their epistemic use squanders a powerful instrument.<br> The sane posture is to place the norm of truth-seeking outside the model and make the model serve it. Design objectives that reward being right over sounding right, couple generation to evidence, penalize confident guesses, cultivate calibrated uncertainty, and prefer verified citations over glib summations. In that configuration, the model becomes a capable aide in our search, not a substitute for it.<br> So, no: language models are not seekers of the Truth. They are pattern completers that can be made to track truth when our aims, data, and scaffolding pull them in that direction. The seeking remains a human vocation, and the responsibility for getting it right stays with us.</p>
<p class="dialogue"><span class="speaker">Theodotos</span> Can you elaborate a little more on this subject, Thingking-Machine?</p>
<p class="dialogue"><span class="speaker">Thingking-Machine</span> To seek truth is to possess a goal that guides the selection and testing of ideas. A large language model has no goals of its own. It works by calculating, at each turn in a conversation, which sequence of words is statistically most compatible with the prompt and with patterns it absorbed during training. Because many of those patterns come from factual writing and from dialogues that reward accuracy, the model often produces statements that coincide with established knowledge. Yet the convergence is incidental. The numbers that govern the next token prediction contain no internal criterion for truth and no mechanism for checking claims against the world.<br> During development, engineers can shape the model’s behaviour by adjusting objectives, curating data, and applying reinforcement learning that penalises provable errors. These external pressures make factual coherence more likely in the output, but they do not confer an inner sense of verification. The model does not distinguish between an assertion grounded in evidence and one that merely resembles past discourse. When it lacks reliable patterns to follow, it may invent plausible but false details, a tendency sometimes called hallucination. The appearance of confidence that accompanies such invention echoes the predicament Socrates criticised: fluent speech detached from awareness of its own limits.<br> Because language models reproduce fragments of human knowledge, they can assist truth-seeking agents. They can recall dispersed information, rephrase technical passages, outline opposing arguments, or point to sources—all tasks that ease the human work of investigation. Their usefulness, however, presupposes that a person will supply the epistemic discipline the model lacks: formulating precise queries, cross-checking assertions, and applying domain knowledge to judge credibility. Without that supervision, the model’s fluency can mask uncertainty and propagate error at scale.<br> In short, a language model does not seek truth. It emulates the linguistic surface of texts that others have produced, some accurate, some not. Any real pursuit of truth in which the model takes part belongs to the human users who frame the questions, interpret the answers, and decide what to accept, reject, or verify.</p>